{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imp import reload\n",
    "import math\n",
    "import h5py\n",
    "import shutil\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import KFold\n",
    "from progiter import ProgIter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mymodule import Trasnformer_model, VAE_model\n",
    "\n",
    "random_state=34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab\n"
     ]
    }
   ],
   "source": [
    "# all drugs\n",
    "with open('/tf/notebooks/code_for_pub/smiles_files/smiles_drugcombANDchembl26.pickle','rb') as f:\n",
    "    a = pickle.load(f)\n",
    "with open('/tf/notebooks/code_for_pub/smiles_files/smiles_drugcomb_BY_cid_duplicated.pickle','rb') as f:\n",
    "    b = pickle.load(f)\n",
    "smiles = a.append(b).drop_duplicates().reset_index(drop=True)\n",
    "v = Trasnformer_model.WordVocab(smiles, max_size=None, min_freq=1)\n",
    "dataset = Trasnformer_model.Seq2seqDataset(smiles, v, seq_len=145)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0/2210... \n",
      "fold: 0, epoch: 0, iter: 0, loss 0.026752619\n",
      "  552/2210... \n",
      "fold: 0, epoch: 0, iter: 552, loss 0.000579581\n",
      " 1104/2210... \n",
      "fold: 0, epoch: 0, iter: 1104, loss 0.000597334\n",
      " 1656/2210... \n",
      "fold: 0, epoch: 0, iter: 1656, loss 0.000254464\n",
      " 2208/2210... \n",
      "fold: 0, epoch: 0, iter: 2208, loss 0.001949811\n",
      " 2209/2210... \n",
      "valid loss: 0.000019487, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:10, now: 1.9487354258312628e-05\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 1, epoch: 0, iter: 0, loss 0.000001160\n",
      "  552/2210... \n",
      "fold: 1, epoch: 0, iter: 552, loss 0.000000558\n",
      " 1104/2210... \n",
      "fold: 1, epoch: 0, iter: 1104, loss 0.000000455\n",
      " 1656/2210... \n",
      "fold: 1, epoch: 0, iter: 1656, loss 0.000000409\n",
      " 2208/2210... \n",
      "fold: 1, epoch: 0, iter: 2208, loss 0.000349793\n",
      " 2209/2210... \n",
      "valid loss: 0.000004099, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:1.9487354258312628e-05, now: 4.09945718366477e-06\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 2, epoch: 0, iter: 0, loss 0.000000362\n",
      "  552/2210... \n",
      "fold: 2, epoch: 0, iter: 552, loss 0.000000448\n",
      " 1104/2210... \n",
      "fold: 2, epoch: 0, iter: 1104, loss 0.000000129\n",
      " 1656/2210... \n",
      "fold: 2, epoch: 0, iter: 1656, loss 0.000000946\n",
      " 2208/2210... \n",
      "fold: 2, epoch: 0, iter: 2208, loss 0.001263027\n",
      " 2209/2210... \n",
      "valid loss: 0.000001047, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:4.09945718366477e-06, now: 1.047461174918679e-06\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 3, epoch: 0, iter: 0, loss 0.000000289\n",
      "  552/2210... \n",
      "fold: 3, epoch: 0, iter: 552, loss 0.000000132\n",
      " 1104/2210... \n",
      "fold: 3, epoch: 0, iter: 1104, loss 0.000000047\n",
      " 1656/2210... \n",
      "fold: 3, epoch: 0, iter: 1656, loss 0.000000151\n",
      " 2208/2210... \n",
      "fold: 3, epoch: 0, iter: 2208, loss 0.001005449\n",
      " 2209/2210... \n",
      "valid loss: 0.000000903, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:1.047461174918679e-06, now: 9.028048594002776e-07\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 4, epoch: 0, iter: 0, loss 0.000000118\n",
      "  552/2210... \n",
      "fold: 4, epoch: 0, iter: 552, loss 0.000000111\n",
      " 1104/2210... \n",
      "fold: 4, epoch: 0, iter: 1104, loss 0.000000029\n",
      " 1656/2210... \n",
      "fold: 4, epoch: 0, iter: 1656, loss 0.000000066\n",
      " 2208/2210... \n",
      "fold: 4, epoch: 0, iter: 2208, loss 0.000817550\n",
      " 2209/2210... \n",
      "valid loss: 0.000000511, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:9.028048594002776e-07, now: 5.106806469925349e-07\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 0, epoch: 1, iter: 0, loss 0.000000137\n",
      "  552/2210... \n",
      "fold: 0, epoch: 1, iter: 552, loss 0.000000078\n",
      " 1104/2210... \n",
      "fold: 0, epoch: 1, iter: 1104, loss 0.000000133\n",
      " 1656/2210... \n",
      "fold: 0, epoch: 1, iter: 1656, loss 0.000000045\n",
      " 2208/2210... \n",
      "fold: 0, epoch: 1, iter: 2208, loss 0.000592816\n",
      " 2209/2210... \n",
      "valid loss: 0.000003229, lr: 0.001\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 1, epoch: 1, iter: 0, loss 0.000000297\n",
      "  552/2210... \n",
      "fold: 1, epoch: 1, iter: 552, loss 0.000000057\n",
      " 1104/2210... \n",
      "fold: 1, epoch: 1, iter: 1104, loss 0.000000013\n",
      " 1656/2210... \n",
      "fold: 1, epoch: 1, iter: 1656, loss 0.000000018\n",
      " 2208/2210... \n",
      "fold: 1, epoch: 1, iter: 2208, loss 0.000301545\n",
      " 2209/2210... \n",
      "valid loss: 0.000002217, lr: 0.001\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 2, epoch: 1, iter: 0, loss 0.000000041\n",
      "  552/2210... \n",
      "fold: 2, epoch: 1, iter: 552, loss 0.000000026\n",
      " 1104/2210... \n",
      "fold: 2, epoch: 1, iter: 1104, loss 0.000000003\n",
      " 1656/2210... \n",
      "fold: 2, epoch: 1, iter: 1656, loss 0.000000032\n",
      " 2208/2210... \n",
      "fold: 2, epoch: 1, iter: 2208, loss 0.000823809\n",
      " 2209/2210... \n",
      "valid loss: 0.000000288, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:5.106806469925349e-07, now: 2.8849517167640306e-07\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 3, epoch: 1, iter: 0, loss 0.000000485\n",
      "  552/2210... \n",
      "fold: 3, epoch: 1, iter: 552, loss 0.000000012\n",
      " 1104/2210... \n",
      "fold: 3, epoch: 1, iter: 1104, loss 0.000000003\n",
      " 1656/2210... \n",
      "fold: 3, epoch: 1, iter: 1656, loss 0.000000011\n",
      " 2208/2210... \n",
      "fold: 3, epoch: 1, iter: 2208, loss 0.000829923\n",
      " 2209/2210... \n",
      "valid loss: 0.000000079, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:2.8849517167640306e-07, now: 7.852093950812746e-08\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 4, epoch: 1, iter: 0, loss 0.000000068\n",
      "  552/2210... \n",
      "fold: 4, epoch: 1, iter: 552, loss 0.000000006\n",
      " 1104/2210... \n",
      "fold: 4, epoch: 1, iter: 1104, loss 0.000000003\n",
      " 1656/2210... \n",
      "fold: 4, epoch: 1, iter: 1656, loss 0.000000002\n",
      " 2208/2210... \n",
      "fold: 4, epoch: 1, iter: 2208, loss 0.000476669\n",
      " 2209/2210... \n",
      "valid loss: 0.000000127, lr: 0.001\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 0, epoch: 2, iter: 0, loss 0.000000112\n",
      "  552/2210... \n",
      "fold: 0, epoch: 2, iter: 552, loss 0.000000057\n",
      " 1104/2210... \n",
      "fold: 0, epoch: 2, iter: 1104, loss 0.000000004\n",
      " 1656/2210... \n",
      "fold: 0, epoch: 2, iter: 1656, loss 0.000000004\n",
      " 2208/2210... \n",
      "fold: 0, epoch: 2, iter: 2208, loss 0.000194731\n",
      " 2209/2210... \n",
      "valid loss: 0.000000619, lr: 0.001\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 1, epoch: 2, iter: 0, loss 0.000000012\n",
      "  552/2210... \n",
      "fold: 1, epoch: 2, iter: 552, loss 0.000000006\n",
      " 1104/2210... \n",
      "fold: 1, epoch: 2, iter: 1104, loss 0.000000010\n",
      " 1656/2210... \n",
      "fold: 1, epoch: 2, iter: 1656, loss 0.000000001\n",
      " 2208/2210... \n",
      "fold: 1, epoch: 2, iter: 2208, loss 0.000115270\n",
      " 2209/2210... \n",
      "valid loss: 0.000000401, lr: 0.001\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 2, epoch: 2, iter: 0, loss 0.000000013\n",
      "  552/2210... \n",
      "fold: 2, epoch: 2, iter: 552, loss 0.000000008\n",
      " 1104/2210... \n",
      "fold: 2, epoch: 2, iter: 1104, loss 0.000000001\n",
      " 1656/2210... \n",
      "fold: 2, epoch: 2, iter: 1656, loss 0.000000004\n",
      " 2208/2210... \n",
      "fold: 2, epoch: 2, iter: 2208, loss 0.000094770\n",
      " 2209/2210... \n",
      "valid loss: 0.000000082, lr: 0.001\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 3, epoch: 2, iter: 0, loss 0.000000117\n",
      "  552/2210... \n",
      "fold: 3, epoch: 2, iter: 552, loss 0.000000003\n",
      " 1104/2210... \n",
      "fold: 3, epoch: 2, iter: 1104, loss 0.000000006\n",
      " 1656/2210... \n",
      "fold: 3, epoch: 2, iter: 1656, loss 0.000000002\n",
      " 2208/2210... \n",
      "fold: 3, epoch: 2, iter: 2208, loss 0.000400829\n",
      " 2209/2210... \n",
      "valid loss: 0.000000164, lr: 0.001\n",
      " 2210/2210... \n",
      "    0/2210... \n",
      "fold: 4, epoch: 2, iter: 0, loss 0.000000048\n",
      "  552/2210... \n",
      "fold: 4, epoch: 2, iter: 552, loss 0.000000007\n",
      " 1104/2210... \n",
      "fold: 4, epoch: 2, iter: 1104, loss 0.000000004\n",
      " 1656/2210... \n",
      "fold: 4, epoch: 2, iter: 1656, loss 0.000000006\n",
      " 2208/2210... \n",
      "fold: 4, epoch: 2, iter: 2208, loss 0.000105326\n",
      " 2209/2210... \n",
      "valid loss: 0.000000011, lr: 0.001\n",
      "currelnt lr: 0.001\n",
      "previous best:7.852093950812746e-08, now: 1.0540242184578913e-08\n",
      " 2210/2210... \n"
     ]
    }
   ],
   "source": [
    "PAD = 0 # size of padding\n",
    "batch_s = 650 # size of molecules per batch\n",
    "num_w = 6 # num_workers for loading data\n",
    "best_loss = 10 # initial random high number above ca. 5\n",
    "n_epoch = 3 # epochs per fold\n",
    "n_layer = 6 # num_layers of transformer\n",
    "patience = 5 # patience of scheduler\n",
    "factor = 0.1 # LR shrink on plateau\n",
    "hidden = 16 # size of fingerprint\n",
    "initial_lr=1e-3  # learning rate\n",
    "n_splits=5 # for CV\n",
    "\n",
    "model = Trasnformer_model.TrfmSeq2seq(\n",
    "    batch_size=batch_s,\n",
    "    in_size=len(v), \n",
    "    hidden_size = hidden,\n",
    "    out_size=len(v),\n",
    "    n_layers=n_layer\n",
    ")\n",
    "m = torch.load('/tf/notebooks/code_for_pub/_logs_as_python_files/transformer_training_logs/16_model_best.pth.tar')\n",
    "model.load_state_dict(m['state_dict'])\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                       factor=factor,\n",
    "                                                       patience=patience,\n",
    "                                                       mode='min', \n",
    "                                                       min_lr=1e-6,\n",
    "                                                      verbose=True)\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "\n",
    "for e in range(0, n_epoch):\n",
    "    for ind,(train_index,test_index) in enumerate(kf.split(dataset)):\n",
    "        train = DataLoader(Subset(dataset, train_index), batch_size=batch_s, shuffle=False, num_workers=num_w)\n",
    "        test = DataLoader(Subset(dataset, test_index), batch_size=batch_s, shuffle=False, num_workers=num_w)\n",
    "        \n",
    "        for b, sm in ProgIter(enumerate(train),total=len(train), verbose=1, show_times=False):\n",
    "            sm = torch.t(sm.cuda()) # (T,B)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(sm) # (T,B,V)\n",
    "\n",
    "            loss = F.nll_loss(output.view(-1, len(v)), sm.contiguous().view(-1), ignore_index = PAD)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if b%(len(train)//4)==0:\n",
    "                print(f'\\nfold: {ind}, epoch: {e}, iter: {b}, loss {loss:.9f}')\n",
    "            if b==len(train)-1:\n",
    "                loss = Trasnformer_model.evaluate(model, test, v, PAD = PAD)\n",
    "                scheduler.step(loss)\n",
    "                print(f'\\nvalid loss: {loss:.9f}, lr: {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "                # Save the model if the validation loss is the best we've seen so far.\n",
    "                is_best = loss < best_loss\n",
    "                if is_best:\n",
    "\n",
    "                    print(f\"currelnt lr: {scheduler.state_dict()['_last_lr'][0]}\")\n",
    "                    print(f\"previous best:{best_loss}, now: {loss}\")\n",
    "                    best_loss = loss\n",
    "                    Trasnformer_model.save_checkpoint({\n",
    "                        'state_dict' : model.state_dict(), \n",
    "                        'best_loss' : best_loss,\n",
    "                        'vocab' : v,\n",
    "                        'lr' : scheduler.state_dict()['_last_lr']},\n",
    "                        is_best, size=hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1843eda38148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# size of padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m280\u001b[0m \u001b[0;31m# size of molecules per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;31m# num_workers for loading data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# initial random high number above ca. 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PAD = 0 # size of padding\n",
    "batch_s = 280 # size of molecules per batch\n",
    "num_w = 6 # num_workers for loading data\n",
    "best_loss = 10 # initial random high number above ca. 5\n",
    "n_epoch = 3 # epochs per fold\n",
    "n_layer = 6 # num_layers of transformer\n",
    "patience = 5 # patience of scheduler\n",
    "factor = 0.1 # LR shrink on plateau\n",
    "hidden = 256 # size of fingerprint\n",
    "initial_lr=1e-4  # learning rate\n",
    "n_splits=5 # for CV\n",
    "\n",
    "model = Trasnformer_model.TrfmSeq2seq(\n",
    "    batch_size=batch_s,\n",
    "    in_size=len(v), \n",
    "    hidden_size = hidden,\n",
    "    out_size=len(v),\n",
    "    n_layers=n_layer\n",
    ")\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                       factor=factor,\n",
    "                                                       patience=patience,\n",
    "                                                       mode='min', \n",
    "                                                       min_lr=1e-6,\n",
    "                                                      verbose=True)\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "\n",
    "for e in range(0, n_epoch):\n",
    "    for ind,(train_index,test_index) in enumerate(kf.split(dataset)):\n",
    "        train = DataLoader(Subset(dataset, train_index), batch_size=batch_s, shuffle=False, num_workers=num_w)\n",
    "        test = DataLoader(Subset(dataset, test_index), batch_size=batch_s, shuffle=False, num_workers=num_w)\n",
    "        \n",
    "        for b, sm in ProgIter(enumerate(train),total=len(train), verbose=1, show_times=False):\n",
    "            sm = torch.t(sm.cuda()) # (T,B)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(sm) # (T,B,V)\n",
    "\n",
    "            loss = F.nll_loss(output.view(-1, len(v)), sm.contiguous().view(-1), ignore_index = PAD)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if b%(len(train)//4)==0:\n",
    "                print(f'\\nfold: {ind}, epoch: {e}, iter: {b}, loss {loss:.5f}')\n",
    "            if b==len(train)-1:\n",
    "                loss = Trasnformer_model.evaluate(model, test, v, PAD = PAD)\n",
    "                scheduler.step(loss)\n",
    "                print(f'\\nvalid loss: {loss:.8f}, lr: {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "                # Save the model if the validation loss is the best we've seen so far.\n",
    "                is_best = loss < best_loss\n",
    "                if is_best:\n",
    "\n",
    "                    print(f\"currelnt lr: {scheduler.state_dict()['_last_lr'][0]}\")\n",
    "                    print(f\"previous best:{best_loss}, now: {loss}\")\n",
    "                    best_loss = loss\n",
    "                    Trasnformer_model.save_checkpoint({\n",
    "                        'state_dict' : model.state_dict(), \n",
    "                        'best_loss' : best_loss,\n",
    "                        'vocab' : v,\n",
    "                        'lr' : scheduler.state_dict()['_last_lr']},\n",
    "                        is_best, size=hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
